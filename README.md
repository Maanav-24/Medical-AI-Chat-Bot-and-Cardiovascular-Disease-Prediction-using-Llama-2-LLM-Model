                               Medical AI Chatbot & Cardiovascular Disease Prediction

# OVERVIEW:

An intelligent, local-first healthcare assistant that combines natural language understanding with medical document analysis and disease risk prediction.

Built using LLaMA, LangChain, FAISS, and Chainlit, this system enables:

ğŸ’¬ Conversational Q&A â€” Chat with an AI-powered medical assistant trained on PDFs and structured health data.

ğŸ“ Smart Document Retrieval â€” Extracts and understands information from PDFs and CSVs for context-rich answers.

ğŸ§  Local LLM Intelligence â€” Uses LLaMA with HuggingFace embeddings and sentence transformers to generate accurate, context-aware responses.

ğŸ©º Health Risk Prediction â€” Capable of integrating models to assess cardiovascular risk from clinical input.

âš¡ Real-Time Interaction â€” Engage with the chatbot through a sleek Chainlit interface.

All components are designed to work locally, ensuring privacy and full control over the pipeline.


## ğŸ§° TECH STACK:

- **ğŸ¦™ LLaMA (via CTransformers)** â€“ Lightweight local LLM for generating answers.
- **ğŸ§  LangChain** â€“ Framework for building retrieval-based QA pipelines.
- **ğŸ“š FAISS** â€“ Vector store for efficient similarity search on embedded text chunks.
- **ğŸ”¤ HuggingFace Embeddings** â€“ Transforms documents into semantic vectors.
- **ğŸ’¬ Sentence Transformers** â€“ Used for high-quality sentence-level embeddings.
- **ğŸ“„ PyPDFLoader & DirectoryLoader** â€“ For loading PDF and CSV (as text) documents.
- **ğŸŒ Chainlit** â€“ Lightweight UI framework for real-time chatbot interaction.
- **ğŸ¼ Pandas** â€“ For reading and processing structured CSV data.
- **ğŸ“¦ Python** â€“ Core language for backend logic and integration.


## âš™ï¸ HOW IT WORKS:

1. Preprocessing: CSV files are read and converted into plain text format. PDFs are also loaded.

2. Embedding Generation: Text chunks are created using a recursive splitter and embedded using Sentence Transformers.

3. Vector Store Creation: FAISS stores these embeddings for efficient retrieval during Q&A.

4. Chat Pipeline:

    a. On query, the system retrieves the most relevant chunks from the vector store.
    b. These chunks are passed to the LLaMA model with a custom prompt.
    c. A final answer is generated and returned through Chainlit's interface.

5. Live Interaction: Chainlit provides real-time communication between the user and the AI system.


## ğŸš€ GETTING STARTED:

1. Place all relevant PDFs and CSVs inside your dataset directory:
C:/MEDIBOT/dataset/

2. Run the script to generate the FAISS vector store.

3. Start the chatbot and interact through the Chainlit UI.

